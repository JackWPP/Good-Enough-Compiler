#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†æˆåˆ†æå™¨Webç•Œé¢

æä¾›è¯æ³•åˆ†æã€è¯­æ³•åˆ†æå’ŒASTæ„å»ºçš„ç»Ÿä¸€Webç•Œé¢ã€‚
åŒ…æ‹¬ï¼š
- æºä»£ç è¾“å…¥å’Œåˆ†æ
- è¯æ³•åˆ†æç»“æœå±•ç¤º
- è¯­æ³•åˆ†æè¿‡ç¨‹æ¼”ç¤º
- ASTå¯è§†åŒ–
- é”™è¯¯æŠ¥å‘Šå’Œè°ƒè¯•ä¿¡æ¯
"""

import gradio as gr
import json
from typing import List, Dict, Any, Optional, Tuple
import traceback

# å¯¼å…¥é›†æˆåˆ†æå™¨
from .integrated_analyzer import (
    IntegratedAnalyzer, AnalysisResult, 
    create_integrated_analyzer, analyze_code_complete
)

# å¯¼å…¥ASTç›¸å…³æ¨¡å—
from .syntax.ast_nodes import ASTVisualizer


class IntegratedWebApp:
    """
    é›†æˆåˆ†æå™¨Webåº”ç”¨ç±»
    """
    
    def __init__(self):
        self.analyzer = None
        self.current_language = 'c'
        self.analysis_result = None
        
    def create_analyzer(self, language: str):
        """åˆ›å»ºåˆ†æå™¨"""
        self.current_language = language
        self.analyzer = create_integrated_analyzer(language)
        return f"å·²åˆ›å»º{language.upper()}è¯­è¨€åˆ†æå™¨"
    
    def analyze_source_code(self, source_code: str, language: str, custom_grammar: str, sentence: str) -> Tuple[str, str, str, str, str, str, str, str]:
        """
        åˆ†ææºä»£ç 
        
        Returns:
            (è¯æ³•åˆ†æç»“æœ, è¯­æ³•åˆ†æç»“æœ, ASTæ ‘å½¢ç»“æ„, ASTå›¾å½¢, åˆ†ææ­¥éª¤, é”™è¯¯ä¿¡æ¯, ç»Ÿè®¡ä¿¡æ¯, è¯­ä¹‰åˆ†æç»“æœ)
        """
        try:
            # åˆ›å»ºæˆ–æ›´æ–°åˆ†æå™¨
            if not self.analyzer or self.current_language != language:
                self.create_analyzer(language)
            
            # è®¾ç½®è‡ªå®šä¹‰æ–‡æ³•
            if custom_grammar.strip():
                self.analyzer.set_grammar(custom_grammar)
            
            # æ‰§è¡Œåˆ†æ
            self.analysis_result = self.analyzer.analyze_code(source_code, sentence)
            
            # æ ¼å¼åŒ–ç»“æœ
            lexical_result = self._format_lexical_result()
            syntax_result = self._format_syntax_result()
            ast_tree = self._format_ast_tree()
            ast_graph = self._format_ast_graph()
            parse_steps = self._format_parse_steps()
            errors = self._format_errors()
            stats = self._format_statistics()
            semantic_output = self._format_semantic_result()
            
            return lexical_result, syntax_result, ast_tree, ast_graph, parse_steps, errors, stats, semantic_output
            
        except Exception as e:
            error_msg = f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯:\n{str(e)}\n\nè¯¦ç»†ä¿¡æ¯:\n{traceback.format_exc()}"
            return "", "", "", "", "", error_msg, "", ""
    
    def _format_lexical_result(self) -> str:
        """æ ¼å¼åŒ–è¯æ³•åˆ†æç»“æœ"""
        if not self.analysis_result or not self.analysis_result.tokens:
            return "æ— è¯æ³•åˆ†æç»“æœ"
        
        lines = []
        lines.append("=== è¯æ³•åˆ†æç»“æœ ===")
        lines.append(f"Tokenæ€»æ•°: {self.analysis_result.token_count}")
        lines.append("-" * 60)
        lines.append(f"{'åºå·':<4} {'ç±»å‹':<20} {'å€¼':<20} {'ä½ç½®':<10}")
        lines.append("-" * 60)
        
        for i, token in enumerate(self.analysis_result.tokens, 1):
            if hasattr(token, 'type') and token.type.value != 'EOF':
                position = f"{getattr(token, 'line', 'N/A')}:{getattr(token, 'column', 'N/A')}"
                lines.append(f"{i:<4} {token.type.value:<20} {str(token.value):<20} {position:<10}")
        
        if self.analysis_result.lexical_errors:
            lines.append("\n=== è¯æ³•é”™è¯¯ ===")
            for error in self.analysis_result.lexical_errors:
                lines.append(f"âŒ {error}")
        
        return "\n".join(lines)
    
    def _format_syntax_result(self) -> str:
        """æ ¼å¼åŒ–è¯­æ³•åˆ†æç»“æœ"""
        if not self.analysis_result:
            return "æ— è¯­æ³•åˆ†æç»“æœ"
        
        lines = []
        lines.append("=== è¯­æ³•åˆ†æç»“æœ ===")
        
        # First/Followé›†
        if self.analysis_result.first_follow:
            lines.append("\n--- First/Followé›† ---")
            lines.append(self.analysis_result.first_follow)
        
        # SLR(1)åˆ¤æ–­
        lines.append("\n--- SLR(1)æ–‡æ³•åˆ¤æ–­ ---")
        lines.append(f"æ˜¯å¦ä¸ºSLR(1)æ–‡æ³•: {'æ˜¯' if self.analysis_result.is_slr1 else 'å¦'}")
        lines.append(self.analysis_result.slr1_result)
        
        # LR(0)çŠ¶æ€
        if self.analysis_result.lr0_states:
            lines.append("\n--- LR(0)çŠ¶æ€ ---")
            lines.append(self.analysis_result.lr0_states[:1000] + "..." if len(self.analysis_result.lr0_states) > 1000 else self.analysis_result.lr0_states)
        
        return "\n".join(lines)
    
    def _format_ast_tree(self) -> str:
        """æ ¼å¼åŒ–ASTæ ‘å½¢ç»“æ„"""
        if not self.analysis_result or not self.analysis_result.ast_tree_string:
            return "æ— ASTç»“æœæˆ–åˆ†æå¤±è´¥"
        
        lines = []
        lines.append("=== æŠ½è±¡è¯­æ³•æ ‘ (AST) ===")
        lines.append("")
        lines.append(self.analysis_result.ast_tree_string)
        
        return "\n".join(lines)
    
    def _format_ast_graph(self) -> str:
        """æ ¼å¼åŒ–ASTå›¾å½¢è¡¨ç¤º"""
        if not self.analysis_result:
            return "<p>æ— ASTç»“æœ</p>"
        
        # ä¼˜å…ˆè¿”å›SVG
        if self.analysis_result.ast_svg:
            return self.analysis_result.ast_svg
        
        # å¦‚æœæ²¡æœ‰SVGï¼Œè¿”å›DOTå›¾
        if self.analysis_result.ast_dot_graph:
            return f"<pre>{self.analysis_result.ast_dot_graph}</pre>"
        
        return "<p>æ— ASTå›¾å½¢è¡¨ç¤º</p>"
    
    def _format_parse_steps(self) -> str:
        """æ ¼å¼åŒ–åˆ†ææ­¥éª¤"""
        if not self.analysis_result or not self.analysis_result.parse_steps:
            return "æ— åˆ†ææ­¥éª¤"
        
        lines = []
        lines.append("=== è¯­æ³•åˆ†ææ­¥éª¤ ===")
        lines.append("-" * 100)
        lines.append(f"{'æ­¥éª¤':<4} {'æ ˆ':<15} {'ç¬¦å·':<20} {'è¾“å…¥':<20} {'åŠ¨ä½œ':<30} {'ASTæ“ä½œ':<15}")
        lines.append("-" * 100)
        
        for step in self.analysis_result.parse_steps:
            if isinstance(step, dict):
                # æ–°æ ¼å¼
                step_num = step.get('step', '')
                stack = step.get('stack', '')
                symbols = step.get('symbols', '')
                input_str = step.get('input', '')
                action = step.get('action', '')
                ast_action = step.get('ast_action', '')
                
                lines.append(f"{str(step_num):<4} {stack:<15} {symbols:<20} {input_str:<20} {action:<30} {ast_action:<15}")
            else:
                # å…¼å®¹æ—§æ ¼å¼
                if len(step) >= 5:
                    lines.append(f"{str(step[0]):<4} {str(step[1]):<15} {str(step[2]):<20} {str(step[3]):<20} {str(step[4]):<30} {'N/A':<15}")
        
        return "\n".join(lines)
    
    def _format_errors(self) -> str:
        """æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯"""
        if not self.analysis_result:
            return "æ— é”™è¯¯ä¿¡æ¯"
        
        lines = []
        
        # è¯æ³•é”™è¯¯
        if self.analysis_result.lexical_errors:
            lines.append("=== è¯æ³•é”™è¯¯ ===")
            for error in self.analysis_result.lexical_errors:
                lines.append(f"âŒ {error}")
            lines.append("")
        
        # è¯­æ³•é”™è¯¯
        if self.analysis_result.syntax_errors:
            lines.append("=== è¯­æ³•é”™è¯¯ ===")
            for error in self.analysis_result.syntax_errors:
                lines.append(f"âŒ {error}")
            lines.append("")

        # è¯­ä¹‰é”™è¯¯
        if hasattr(self.analysis_result, 'semantic_errors') and self.analysis_result.semantic_errors:
            lines.append("=== è¯­ä¹‰é”™è¯¯ ===")
            for error in self.analysis_result.semantic_errors:
                lines.append(f"âŒ {error}")
            lines.append("")
        
        if not lines:
            lines.append("âœ… æ— é”™è¯¯")
        
        return "\n".join(lines)

    def _format_semantic_result(self) -> str:
        """æ ¼å¼åŒ–è¯­ä¹‰åˆ†æç»“æœ"""
        if not self.analysis_result:
            return "æ— è¯­ä¹‰åˆ†æç»“æœ"

        lines = []
        lines.append("=== è¯­ä¹‰åˆ†æç»“æœ ===")

        if hasattr(self.analysis_result, 'semantic_result') and self.analysis_result.semantic_result is not None:
            lines.append(f"æœ€ç»ˆè¯­ä¹‰ç»“æœ: {str(self.analysis_result.semantic_result)}")
        else:
            lines.append("æœªç”Ÿæˆæœ€ç»ˆè¯­ä¹‰ç»“æœæˆ–åˆ†æå¤±è´¥")
        
        lines.append("\n--- ç¬¦å·è¡¨ ---")
        if hasattr(self.analysis_result, 'symbol_table_string') and self.analysis_result.symbol_table_string:
            lines.append(self.analysis_result.symbol_table_string)
        else:
            lines.append("æ— ç¬¦å·è¡¨ä¿¡æ¯")
        
        return "\n".join(lines)
    
    def _format_statistics(self) -> str:
        """æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.analysis_result:
            return "æ— ç»Ÿè®¡ä¿¡æ¯"
        
        lines = []
        lines.append("=== åˆ†æç»Ÿè®¡ ===")
        lines.append(f"Tokenæ•°é‡: {self.analysis_result.token_count}")
        lines.append(f"åˆ†ææ—¶é—´: {self.analysis_result.analysis_time:.4f}ç§’")
        lines.append(f"åˆ†æçŠ¶æ€: {'æˆåŠŸ' if self.analysis_result.success else 'å¤±è´¥'}")
        lines.append(f"æ˜¯å¦ä¸ºSLR(1): {'æ˜¯' if self.analysis_result.is_slr1 else 'å¦'}")
        
        # ASTç»Ÿè®¡
        if self.analysis_result.ast_root:
            lines.append(f"ASTèŠ‚ç‚¹: å·²ç”Ÿæˆ")
            lines.append(f"ASTå¯è§†åŒ–: {'å¯ç”¨' if self.analysis_result.ast_svg else 'ä¸å¯ç”¨'}")
        else:
            lines.append(f"ASTèŠ‚ç‚¹: æœªç”Ÿæˆ")
        
        return "\n".join(lines)


def create_web_interface():
    """
    åˆ›å»ºGradio Webç•Œé¢
    """
    app = IntegratedWebApp()
    
    # ç¤ºä¾‹ä»£ç 
    example_c_code = """int main() {
    int a = 5;
    int b = 3;
    int c = a + b;
    return 0;
}"""
    
    example_pascal_code = """program example;
var
    a, b, c: integer;
begin
    a := 5;
    b := 3;
    c := a + b;
end."""
    
    example_grammar = """E â†’ E + T | E - T | T
T â†’ T * F | T / F | F
F â†’ ( E ) | id | num"""
    
    with gr.Blocks(title="é›†æˆç¼–è¯‘å™¨åˆ†æå™¨ - æ”¯æŒASTç”Ÿæˆ", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ğŸ”¬ é›†æˆç¼–è¯‘å™¨åˆ†æå™¨")
        gr.Markdown("**åŠŸèƒ½**: è¯æ³•åˆ†æ + è¯­æ³•åˆ†æ + ASTç”Ÿæˆ + å¯è§†åŒ–")
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“ è¾“å…¥é…ç½®")
                
                language = gr.Dropdown(
                    choices=["c", "pascal"], 
                    value="c", 
                    label="ç¼–ç¨‹è¯­è¨€"
                )
                
                source_code = gr.Textbox(
                    lines=10,
                    placeholder="è¯·è¾“å…¥æºä»£ç ...",
                    label="æºä»£ç ",
                    value=example_c_code
                )
                
                custom_grammar = gr.Textbox(
                    lines=6,
                    placeholder="å¯é€‰ï¼šè‡ªå®šä¹‰æ–‡æ³•è§„åˆ™...",
                    label="è‡ªå®šä¹‰æ–‡æ³•ï¼ˆå¯é€‰ï¼‰",
                    value=example_grammar
                )
                
                sentence = gr.Textbox(
                    placeholder="å¯é€‰ï¼šå¾…åˆ†æçš„å¥å­ï¼ˆå¦‚ï¼šid + num * idï¼‰",
                    label="æµ‹è¯•å¥å­ï¼ˆå¯é€‰ï¼‰",
                    value="id + num * id"
                )
                
                analyze_btn = gr.Button("ğŸš€ å¼€å§‹åˆ†æ", variant="primary")
            
            with gr.Column(scale=2):
                gr.Markdown("## ğŸ“Š åˆ†æç»“æœ")
                
                with gr.Tabs():
                    with gr.TabItem("è¯æ³•åˆ†æ"):
                        lexical_output = gr.Textbox(
                            lines=15,
                            label="è¯æ³•åˆ†æç»“æœ",
                            show_copy_button=True
                        )
                    
                    with gr.TabItem("è¯­æ³•åˆ†æ"):
                        syntax_output = gr.Textbox(
                            lines=15,
                            label="è¯­æ³•åˆ†æç»“æœ",
                            show_copy_button=True
                        )
                    
                    with gr.TabItem("ASTæ ‘å½¢ç»“æ„"):
                        ast_tree_output = gr.Textbox(
                            lines=15,
                            label="ASTæ ‘å½¢ç»“æ„",
                            show_copy_button=True
                        )
                    
                    with gr.TabItem("ASTå›¾å½¢"):
                        ast_graph_output = gr.HTML(
                            label="ASTå›¾å½¢è¡¨ç¤º"
                        )
                    
                    with gr.TabItem("åˆ†ææ­¥éª¤"):
                        steps_output = gr.Textbox(
                            lines=15,
                            label="è¯­æ³•åˆ†ææ­¥éª¤",
                            show_copy_button=True
                        )
                    
                    with gr.TabItem("é”™è¯¯ä¿¡æ¯"):
                        errors_output = gr.Textbox(
                            lines=10,
                            label="é”™è¯¯ä¿¡æ¯"
                        )
                    
                    with gr.TabItem("ç»Ÿè®¡ä¿¡æ¯"):
                        stats_output = gr.Textbox(
                            lines=10,
                            label="ç»Ÿè®¡ä¿¡æ¯"
                        )
                    
                    with gr.TabItem("è¯­ä¹‰åˆ†æ"):
                        semantic_output = gr.Textbox(
                            lines=15,
                            label="è¯­ä¹‰åˆ†æç»“æœ",
                            show_copy_button=True
                        )
        
        # ç¤ºä¾‹æŒ‰é’®
        with gr.Row():
            gr.Markdown("### ğŸ“š ç¤ºä¾‹")
        
        with gr.Row():
            example_c_btn = gr.Button("Cè¯­è¨€ç¤ºä¾‹")
            example_pascal_btn = gr.Button("Pascalç¤ºä¾‹")
            example_expr_btn = gr.Button("è¡¨è¾¾å¼ç¤ºä¾‹")
        
        # äº‹ä»¶ç»‘å®š
        analyze_btn.click(
            fn=app.analyze_source_code,
            inputs=[source_code, language, custom_grammar, sentence],
            outputs=[lexical_output, syntax_output, ast_tree_output, ast_graph_output, steps_output, errors_output, stats_output, semantic_output]
        )
        
        # ç¤ºä¾‹æŒ‰é’®äº‹ä»¶
        example_c_btn.click(
            lambda: ("c", example_c_code, example_grammar, "id + num"),
            outputs=[language, source_code, custom_grammar, sentence]
        )
        
        example_pascal_btn.click(
            lambda: ("pascal", example_pascal_code, "", "id := num"),
            outputs=[language, source_code, custom_grammar, sentence]
        )
        
        example_expr_btn.click(
            lambda: ("c", "a + b * c", example_grammar, "id + id * id"),
            outputs=[language, source_code, custom_grammar, sentence]
        )
        
        # è¯­è¨€åˆ‡æ¢äº‹ä»¶
        language.change(
            lambda lang: example_c_code if lang == "c" else example_pascal_code,
            inputs=[language],
            outputs=[source_code]
        )
    
    return interface


def main():
    """
    ä¸»å‡½æ•°ï¼šå¯åŠ¨Webåº”ç”¨
    """
    print("ğŸš€ å¯åŠ¨é›†æˆåˆ†æå™¨Webç•Œé¢...")
    
    try:
        # æµ‹è¯•æ¨¡å—å¯¼å…¥
        print("ğŸ“¦ æµ‹è¯•æ¨¡å—å¯¼å…¥...")
        from .integrated_analyzer import create_integrated_analyzer
        analyzer = create_integrated_analyzer('c')
        print("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # åˆ›å»ºWebç•Œé¢
        print("ğŸŒ åˆ›å»ºWebç•Œé¢...")
        interface = create_web_interface()
        print("âœ… Webç•Œé¢åˆ›å»ºæˆåŠŸ")
        
        # å¯åŠ¨æœåŠ¡å™¨
        print("ğŸ”¥ å¯åŠ¨æœåŠ¡å™¨...")
        interface.launch(
            server_name="127.0.0.1",
            server_port=7861,
            share=False,
            quiet=False,
            show_error=True
        )
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨å…³é—­æœåŠ¡å™¨...")
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")


if __name__ == "__main__":
    main()